{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook introduces items for U.S. States into the knowledgebase. This later iteration on the notebook works from the transformation of U.S. Census data on the Microsoft Planetary Computer vs. my initial approach using the HTML tables directly from the U.S. Census' TIGER system. The MPC instantiation on Azure public cloud storage provides a more robust processing environment for when we need to run geospatial operations with the data.\n",
    "\n",
    "Every time I come back to a given reference source, I work through a few improvements in thinking about how to incorporate the material into the knowledge graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "178b7d1eec2741c4ac8de3eab8e5817a",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 177,
    "execution_start": 1678044091093,
    "source_hash": "d8e33b87",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import planetary_computer\n",
    "import pystac_client\n",
    "import dask_geopandas\n",
    "import pandas as pd\n",
    "\n",
    "from wbmaker import WikibaseConnection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "eew = WikibaseConnection('EEW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select source item to process\n",
    "source_item_id = eew.ref_lookup['U.S. State names and identifiers from U.S. Census via Microsoft Planetary Computer']\n",
    "source_item = eew.wbi.item.get(source_item_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data source documentation\n",
    "\n",
    "I took another approach toward documenting a data source here with some new properties that should be applicable for other Spatiotemporal Asset Catalog (STAC) sources. We need to know essentially three pieces of information - a catalog URL, collection name, and item name. From those, we can pull the necessary details to get a data source. This applies to a parquet file format, but other types of assets may just need a different method for reading and working with the data.\n",
    "\n",
    "The other thing I've worked on in the item is the mapping of source properties to knowledgebase properties. In this instance, I didn't use that mapping to drive the processing, but I'll come back and look at that again at some point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract STAC connection details from source item\n",
    "stac_source = source_item.claims.get_json()[eew.prop_lookup['STAC catalog URL']][0]\n",
    "stac_catalog_url = stac_source['mainsnak']['datavalue']['value']\n",
    "stac_collection_name = stac_source['qualifiers'][eew.prop_lookup['STAC Collection Name']][0]['datavalue']['value']\n",
    "stac_item_name = stac_source['qualifiers'][eew.prop_lookup['STAC Item Name']][0]['datavalue']['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query STAC catalog for asset\n",
    "stac_catalog = pystac_client.Client.open(\n",
    "    stac_catalog_url,\n",
    "    modifier=planetary_computer.sign_inplace,\n",
    ")\n",
    "stac_collection = stac_catalog.get_collection(stac_collection_name)\n",
    "stac_asset = stac_collection.get_item(stac_item_name).assets[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read source data and compute coordinates\n",
    "gdf_us_states = dask_geopandas.read_parquet(\n",
    "    stac_asset.href,\n",
    "    storage_options=stac_asset.extra_fields[\"table:storage_options\"],\n",
    "    calculate_divisions=True,\n",
    ")\n",
    "gdf_us_states = gdf_us_states.to_crs(epsg=4326).compute()\n",
    "gdf_us_states['coordinates'] = gdf_us_states.to_crs('+proj=cea').geometry.centroid.to_crs(gdf_us_states.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lookup QID for LSAD and merge with source data\n",
    "query_lsad = \"PREFIX%20wdt%3A%20%3Chttps%3A%2F%2Feew-edgi.wikibase.cloud%2Fprop%2Fdirect%2F%3E%0A%0ASELECT%20%3Fitem%20%3Flsad_code%0AWHERE%20%7B%0A%20%20%3Fitem%20wdt%3AP54%20%3Flsad_code%20.%0A%7D%0A\"\n",
    "df_lsad = eew.wb_ref_data(query=query_lsad)\n",
    "df_lsad['lsad_qid'] = df_lsad.item.apply(lambda x: x.split('/')[-1])\n",
    "\n",
    "state_records = pd.merge(\n",
    "    left=gdf_us_states,\n",
    "    right=df_lsad[[\"lsad_code\",\"lsad_qid\"]],\n",
    "    how=\"left\",\n",
    "    left_on=\"LSAD\",\n",
    "    right_on=\"lsad_code\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the instance of for the item based on FIPS code\n",
    "def instance_of_name(STATEFP):\n",
    "    int_statefp = int(STATEFP)\n",
    "    if int_statefp == 11:\n",
    "        return 'U.S. federal district'\n",
    "    if int_statefp > 56:\n",
    "        return 'U.S. Territory'\n",
    "    return 'U.S. State'\n",
    "\n",
    "state_records['instance_of_name'] = state_records.STATEFP.apply(instance_of_name)\n",
    "state_records['instance_of_qid'] = state_records.instance_of_name.apply(lambda x: eew.class_lookup[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_state(fips_alpha):\n",
    "    q = \"\"\"\n",
    "    %(namespaces)s\n",
    "\n",
    "    SELECT ?st ?fips_alpha\n",
    "    WHERE {\n",
    "    ?st wdt:%(p_fips_alpha)s \"%(v_fips_alpha)s\" .\n",
    "    ?st wdt:%(p_fips_alpha)s ?fips_alpha .\n",
    "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\" . }\n",
    "    }\n",
    "    \"\"\" % {\n",
    "        \"namespaces\": eew.sparql_namespaces(),\n",
    "        \"v_fips_alpha\": fips_alpha,\n",
    "        \"p_fips_alpha\": eew.prop_lookup['FIPS 5-2 alpha']\n",
    "    }\n",
    "\n",
    "    return eew.sparql_query(query=q, output=\"lookup\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item upsert\n",
    "\n",
    "This is a somewhat new approach I'm trying where I check for an existing item based on some specific identifier and then either pull that item to work on or instantiate a new item. Everything else follows in establishing claims from the specific source and even setting labels, descriptions, and aliases. WBI includes a \"clear\" parameter on the write operation, which will clear existing claims that align directly on prop_nr and value. It leaves in place any other claims that might exist on the item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATED: Guam Q242\n",
      "UPDATED: Texas Q311\n",
      "UPDATED: Wisconsin Q317\n",
      "UPDATED: Rhode Island Q307\n",
      "UPDATED: New York Q300\n",
      "UPDATED: United States Virgin Islands Q152\n",
      "UPDATED: New Hampshire Q297\n",
      "UPDATED: Minnesota Q291\n",
      "UPDATED: Puerto Rico Q149\n",
      "UPDATED: Missouri Q293\n",
      "UPDATED: North Carolina Q301\n",
      "UPDATED: Michigan Q290\n",
      "UPDATED: Louisiana Q286\n",
      "UPDATED: Nebraska Q295\n",
      "UPDATED: California Q272\n",
      "UPDATED: Wyoming Q318\n",
      "UPDATED: South Carolina Q308\n",
      "UPDATED: Commonwealth of the Northern Mariana Islands Q59\n",
      "UPDATED: Kansas Q284\n",
      "UPDATED: Delaware Q275\n",
      "UPDATED: Alaska Q269\n",
      "UPDATED: New Jersey Q298\n",
      "UPDATED: North Dakota Q302\n",
      "UPDATED: District of Columbia Q276\n",
      "UPDATED: Colorado Q273\n",
      "UPDATED: Virginia Q314\n",
      "UPDATED: Indiana Q282\n",
      "UPDATED: Nevada Q296\n",
      "UPDATED: New Mexico Q299\n",
      "UPDATED: Alabama Q268\n",
      "UPDATED: Tennessee Q310\n",
      "UPDATED: Kentucky Q285\n",
      "UPDATED: Oregon Q305\n",
      "UPDATED: Mississippi Q292\n",
      "UPDATED: Connecticut Q274\n",
      "UPDATED: Georgia Q278\n",
      "UPDATED: Utah Q312\n",
      "UPDATED: Idaho Q280\n",
      "UPDATED: Illinois Q281\n",
      "UPDATED: Iowa Q283\n",
      "UPDATED: Arizona Q270\n",
      "UPDATED: American Samoa Q243\n",
      "UPDATED: Vermont Q313\n",
      "UPDATED: Montana Q294\n",
      "UPDATED: South Dakota Q309\n",
      "UPDATED: Pennsylvania Q306\n",
      "UPDATED: Oklahoma Q304\n",
      "UPDATED: Maryland Q288\n",
      "UPDATED: Maine Q287\n",
      "UPDATED: Hawaii Q279\n",
      "UPDATED: Ohio Q303\n",
      "UPDATED: West Virginia Q316\n",
      "UPDATED: Washington Q315\n",
      "UPDATED: Arkansas Q271\n",
      "UPDATED: Massachusetts Q289\n",
      "UPDATED: Florida Q277\n"
     ]
    }
   ],
   "source": [
    "references = eew.models.References()\n",
    "references.add(\n",
    "    eew.datatypes.Item(\n",
    "        prop_nr=eew.prop_lookup['data source'],\n",
    "        value=source_item_id\n",
    "    )\n",
    ")\n",
    "\n",
    "for index, row in state_records.iterrows():\n",
    "    wb_item_id = lookup_state(row.STUSPS)\n",
    "    if wb_item_id:\n",
    "        wb_item = eew.wbi.item.get(wb_item_id[row.STUSPS])\n",
    "    else:\n",
    "        wb_item = eew.wbi.item.new()\n",
    "\n",
    "    wb_item.labels.set('en', row.NAME)\n",
    "    wb_item.aliases.set('en', row.STUSPS)\n",
    "    wb_item.descriptions.set('en', f'a {row.instance_of_name}')\n",
    "\n",
    "    claims = eew.models.Claims()\n",
    "    claims.add(\n",
    "        eew.datatypes.Item(\n",
    "            prop_nr=eew.prop_lookup['instance of'],\n",
    "            value=row.instance_of_qid,\n",
    "            references=references\n",
    "        )\n",
    "    )\n",
    "    claims.add(\n",
    "        eew.datatypes.ExternalID(\n",
    "            prop_nr=eew.prop_lookup['FIPS 5-2 alpha'],\n",
    "            value=row.STUSPS,\n",
    "            references=references\n",
    "        )\n",
    "    )\n",
    "    claims.add(\n",
    "        eew.datatypes.ExternalID(\n",
    "            prop_nr=eew.prop_lookup['FIPS 5-2 numeric'],\n",
    "            value=str(row.STATEFP),\n",
    "            references=references\n",
    "        )\n",
    "    )\n",
    "    claims.add(\n",
    "        eew.datatypes.ExternalID(\n",
    "            prop_nr=eew.prop_lookup['FIPS 10-4'],\n",
    "            value=f\"US{row.STATEFP}\",\n",
    "            references=references\n",
    "        )\n",
    "    )\n",
    "    claims.add(\n",
    "        eew.datatypes.ExternalID(\n",
    "            prop_nr=eew.prop_lookup['ISO 3166-2 code'],\n",
    "            value=f\"US-{row.STUSPS}\",\n",
    "            references=references\n",
    "        )\n",
    "    )\n",
    "    claims.add(\n",
    "        eew.datatypes.ExternalID(\n",
    "            prop_nr=eew.prop_lookup['TIGER GEOID'],\n",
    "            value=str(row.GEOID),\n",
    "            references=references\n",
    "        )\n",
    "    )\n",
    "    claims.add(\n",
    "        eew.datatypes.ExternalID(\n",
    "            prop_nr=eew.prop_lookup['GNIS ID'],\n",
    "            value=str(row.STATENS),\n",
    "            references=references\n",
    "        )\n",
    "    )\n",
    "    claims.add(\n",
    "        eew.datatypes.GlobeCoordinate(\n",
    "            prop_nr=eew.prop_lookup['coordinate location'],\n",
    "            latitude=row.coordinates.y,\n",
    "            longitude=row.coordinates.x,\n",
    "            references=references\n",
    "        )\n",
    "    )\n",
    "    claims.add(\n",
    "        eew.datatypes.Item(\n",
    "            prop_nr=eew.prop_lookup['Legal/Statistical Area Description'],\n",
    "            value=row.lsad_qid,\n",
    "            references=references\n",
    "        )\n",
    "    )\n",
    "\n",
    "    wb_item.add_claims(claims=claims)\n",
    "    response = wb_item.write(\n",
    "        summary=\"Updated item from Census source\",\n",
    "        clear=True\n",
    "    )\n",
    "    print(\"UPDATED:\", row.NAME, response.id)\n"
   ]
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "72dfe86cf9c941cd84c562b05cb374aa",
  "deepnote_persisted_session": {
   "createdAt": "2023-03-05T20:01:49.619Z"
  },
  "kernelspec": {
   "display_name": "Python [conda env:notebook] *",
   "language": "python",
   "name": "conda-env-notebook-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
