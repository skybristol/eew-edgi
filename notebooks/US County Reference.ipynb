{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook works through the process of adding records for U.S. Counties (and equivalent units) to the knowledgebase. I'm starting with the U.S. Census TIGER source that I've laid out in a data source item in the knowledgebase already.\n",
    "\n",
    "This is an interesting case in that the information in Wikidata is probably pretty good. There are items for U.S. Counties, and Wikidata contributors have gone so far as to build out crosslinks from states to their counties and from counties to their state. However, this is some level of semantic dissonance in how these items are classified, and we have to consult a lot of information to decide if we're going to trust the completeness of the information in Wikidata. So, I'm going to start with a more authoritative source from which to base items, establish linkages to Wikidata via identifiers, and then decide what I can do with those relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "178b7d1eec2741c4ac8de3eab8e5817a",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 177,
    "execution_start": 1678044091093,
    "source_hash": "d8e33b87",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from functions import (\n",
    "    sparql_query,\n",
    "    kb_props,\n",
    "    kb_datasources,\n",
    "    valid_classes\n",
    ")\n",
    "\n",
    "from wikibaseintegrator.wbi_config import config as wbi_config\n",
    "from wikibaseintegrator import WikibaseIntegrator, wbi_login\n",
    "from wikibaseintegrator.models import Qualifiers, References, Reference, Claims\n",
    "from wikibaseintegrator import datatypes\n",
    "from wikibaseintegrator.wbi_helpers import execute_sparql_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_id": "c569b615e60c41e5a116a9afa68d9608",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2211,
    "execution_start": 1678044095791,
    "source_hash": "1a363e56",
    "tags": []
   },
   "outputs": [],
   "source": [
    "wbi_config['MEDIAWIKI_API_URL'] = os.environ['MEDIAWIKI_API_URL']\n",
    "wbi_config['SPARQL_ENDPOINT_URL'] = os.environ['SPARQL_ENDPOINT_URL']\n",
    "wbi_config['WIKIBASE_URL'] = os.environ['WIKIBASE_URL']\n",
    "wbi_config['USER_AGENT'] = f'EDJIBot/1.0 ({os.environ[\"WIKIBASE_URL\"]})'\n",
    "\n",
    "login_instance = wbi_login.Login(\n",
    "    user=os.environ['BOT_NAME'],\n",
    "    password=os.environ['BOT_PASS']\n",
    ")\n",
    "\n",
    "wbi = WikibaseIntegrator(login=login_instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundational properties, classifiers, and data sources\n",
    "\n",
    "Every time we run a workflow to build out some concept in the knowledgebase, we need to pull a reference together of the fundamental properties and specific definition information that drives how claims are built, the items that serve as classifiers (establishing \"instance of\" claims), and data sources. As I work through each source several times, I'm fiddling with the best way to document a source such that a link to that item in a reference from a claim provides a lot of detail to fully understand where the claim came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_id": "e47d3569942a4ead8742d593663c4487",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 7893,
    "execution_start": 1678044101129,
    "source_hash": "1ccbf813",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prop_item_definitions, properties = kb_props()\n",
    "classes = valid_classes()\n",
    "datasources = kb_datasources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NAICS Sector Code': 'P6',\n",
       " 'NAICS Subsector Code': 'P7',\n",
       " 'NAICS Industry Group Code': 'P8',\n",
       " 'NAICS Industry Code': 'P9',\n",
       " 'NAICS National Industry Code': 'P10',\n",
       " 'instance of': 'P1',\n",
       " 'subclass of': 'P2',\n",
       " 'SIC Code': 'P3',\n",
       " 'reference url': 'P4',\n",
       " 'data source': 'P5',\n",
       " 'file format': 'P11',\n",
       " 'item of this property': 'P12',\n",
       " 'identifier length': 'P13',\n",
       " 'formatter URL': 'P14',\n",
       " 'equivalent property': 'P15',\n",
       " 'related wikidata item': 'P16',\n",
       " 'caveat': 'P17',\n",
       " 'ISO 3166-1 alpha-2 code': 'P18',\n",
       " 'ISO 3166-1 alpha-3 code': 'P19',\n",
       " 'ISO 3166-1 numeric code': 'P20',\n",
       " 'ISO 3166-2 code': 'P21',\n",
       " 'country': 'P22',\n",
       " 'location': 'P23',\n",
       " 'HTML Data Table': 'P24',\n",
       " 'entity classification': 'P26',\n",
       " 'FIPS 5-2 alpha code': 'P27',\n",
       " 'GNIS ID': 'P28',\n",
       " 'coordinate location': 'P29',\n",
       " 'FIPS 5-2 numeric code': 'P30',\n",
       " 'FIPS 6-4': 'P31'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'spatio-temporal activity': 'Q2',\n",
       " 'data source': 'Q4',\n",
       " 'file format': 'Q455',\n",
       " 'geographic entity': 'Q2148',\n",
       " 'industrial activity': 'Q3',\n",
       " 'NAICS Sector': 'Q450',\n",
       " 'NAICS Subsector': 'Q451',\n",
       " 'NAICS Industry Group': 'Q452',\n",
       " 'NAICS Industry': 'Q453',\n",
       " 'NAICS Industry (national)': 'Q454',\n",
       " 'artificial geographic entity': 'Q2149',\n",
       " 'country': 'Q1897',\n",
       " 'U.S. State': 'Q2150',\n",
       " 'U.S. Territory': 'Q2158',\n",
       " 'U.S. County': 'Q2206',\n",
       " 'civil political division': 'Q2207'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'SEC listing of SIC codes': 'Q5',\n",
       " 'North American Industry Classification System': 'Q458',\n",
       " 'U.S. Census Bureau TIGER Data Files': 'Q2205',\n",
       " 'Wikidata': 'Q2208'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(properties)\n",
    "display(classes)\n",
    "display(datasources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIGER Source\n",
    "\n",
    "I'm starting off working against the very rudimentary but complete set of data tables for counties (and equivalent units) from the U.S. Census web site as HTML tables. On the one hand, this seems like a technologically crude source to operate against when there are other options like the ArcGIS REST services also advertised by Census. County names and identifiers are also not all that special in that they are incorporated into any number of other data services, some of which are more functionally usable than what Census puts out as part of TIGER.\n",
    "\n",
    "However, exploring the available services left some confusion as to what I should actually use as a foundational data source. As is typical in many cases, the ArcGIS services are organized in a way that supports GIS and web mapping functionality specific to how Census operates. This means they are not really spun up with the intent of serving other use cases. They could change based on what the provider's needs are, and there are actually many \"layers\" that provide exactly the same information on U.S. Counties - which one do I use?\n",
    "\n",
    "So, even though the HTML tables are crude, they are simple enough to read and parse for use. There could be issues like text encoding of extra spaces that have to be dealt with, but we can check for those easily enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiger_source_query = \"\"\"\n",
    "PREFIX wd: <https://edji-knows.wikibase.cloud/entity/>\n",
    "PREFIX wdt: <https://edji-knows.wikibase.cloud/prop/direct/>\n",
    "\n",
    "SELECT ?statement\n",
    "WHERE {\n",
    "  wd:Q2205 wdt:P24 ?statement.\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\" . }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "tiger_source_urls = sparql_query(\n",
    "    endpoint=os.environ['SPARQL_ENDPOINT_URL'],\n",
    "    query=tiger_source_query,\n",
    "    output=\"dict\"\n",
    ")\n",
    "\n",
    "county_source_links = [i for i in tiger_source_urls if \"_county_\" in i[\"statement\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.74 s, sys: 27.8 ms, total: 2.77 s\n",
      "Wall time: 28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "county_sources = []\n",
    "\n",
    "for source_link in county_source_links:\n",
    "    url = source_link['statement']\n",
    "    source_dfs = pd.read_html(\n",
    "        url,\n",
    "        converters={\n",
    "            'GEOID': str, \n",
    "            'STATE': str,\n",
    "            'COUNTY': str,\n",
    "            'COUNTYNS': str \n",
    "        }\n",
    "    )\n",
    "    county_sources.append(source_dfs[0])\n",
    "\n",
    "df_county_sources = pd.concat(county_sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Here\n",
    "\n",
    "* NAME and BASENAME can be used as label and an alias\n",
    "* GEOID is the FIPS 6-4 ExternalID (new property from this source)\n",
    "* COUNTYNS is the GNIS ID\n",
    "* CENTLAT and CENTLON give us coordinate location\n",
    "* STATE provides our link to existing U.S. State or U.S. Territory items in our knowledgebase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bring in States\n",
    "\n",
    "Since we need to establish a linkage to state items, we can pull in the state name and abbreviation to build out additional aliases on these items. In establishing the linkage to state, I'm currently sticking with a much simpler classification of the relationship and making this a \"location/located in\" relationship. This is crude and may need to get more sophisticated, but I don't know what the most appropriate semantics are at this point. Wikidata uses \"located in the administrative territorial entity,\" which makes sense, but there are other ontologies that might better inform these relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_kb_states_territories = \"\"\"\n",
    "PREFIX wd: <https://edji-knows.wikibase.cloud/entity/>\n",
    "PREFIX wdt: <https://edji-knows.wikibase.cloud/prop/direct/>\n",
    "\n",
    "SELECT ?st ?stLabel ?fips_alpha ?STATE\n",
    "WHERE {\n",
    "    ?st wdt:P1 ?classifier .\n",
    "    VALUES ?classifier { wd:Q2150 wd:Q2158 } .\n",
    "    OPTIONAL { ?st wdt:P27 ?fips_alpha . }\n",
    "    OPTIONAL { ?st wdt:P30 ?STATE . }\n",
    "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\" . }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "kb_states_territories = sparql_query(\n",
    "    endpoint=os.environ['SPARQL_ENDPOINT_URL'],\n",
    "    query=query_kb_states_territories,\n",
    "    output=\"dataframe\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_counties_states = pd.merge(\n",
    "    left=df_county_sources,\n",
    "    right=kb_states_territories,\n",
    "    how=\"left\",\n",
    "    on=\"STATE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a label that will be unique by combining county name and state name\n",
    "merged_counties_states['label'] = merged_counties_states.NAME + ', ' + merged_counties_states.stLabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikidata Items with FIPS Codes\n",
    "\n",
    "Our best way to connect \"county\" items in this knowledgebase to Wikidata items that might align and contain additional relevant information and linkages is through the FIPS 6-4 codes. We can query for everything in Wikidata with that property and return information to work with.\n",
    "\n",
    "One interesting challenge with the FIPS Coded items at the \"county\" level is that these are not all counties. Classification of these areas below the U.S. State level gets messy. Contributors to Wikidata took the tact of building state-level classification items to some extent with a mix of other things. Excluding names that end in \"County,\" still yields 24 different type classifications. To simplify this for our purposes, I create a new property for a \"civil political division\" and used that to classify everything with a FIPS 6-4 code. If we need to get into deeper level classification to handle particular use cases, we can perhaps pull concepts from Wikidata or another source.\n",
    "\n",
    "Since descriptions can be just about anything and the information coming from Wikidata for FIPS 6-4 identified items may be somewhat useful in the absence of specific classification, I opted to use those descriptions in building our items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_wd_fips64 = \"\"\"\n",
    "SELECT ?item ?itemLabel ?itemDescription ?GEOID \n",
    "(GROUP_CONCAT(?classLabel  ; separator=',') as ?classes)\n",
    "WHERE {\n",
    "  ?item wdt:P882 ?GEOID .\n",
    "  ?item wdt:P31 ?class .\n",
    "  ?class rdfs:label ?classLabel . filter (lang(?classLabel)='en')\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "}\n",
    "GROUP BY ?item ?itemLabel ?itemDescription ?GEOID\n",
    "\"\"\"\n",
    "\n",
    "wd_fips64 = sparql_query(\n",
    "    endpoint=\"https://query.wikidata.org/sparql\",\n",
    "    query=query_wd_fips64,\n",
    "    output=\"dataframe\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['county seat',\n",
       " 'independent city',\n",
       " 'railway town',\n",
       " 'parish of Louisiana',\n",
       " 'borough of Alaska',\n",
       " 'Metropolitan Statistical Area',\n",
       " 'municipality of Puerto Rico',\n",
       " 'big city',\n",
       " 'city in the United States',\n",
       " 'consolidated city-county',\n",
       " 'state or insular area capital of the United States',\n",
       " 'census area of Alaska',\n",
       " 'city',\n",
       " 'census-designated place in the United States',\n",
       " 'island',\n",
       " 'unorganized atoll of American Samoa',\n",
       " 'disputed territory',\n",
       " 'unincorporated territory',\n",
       " 'insular area',\n",
       " 'territory of the United States',\n",
       " 'municipality of the Northern Mariana Islands',\n",
       " 'district of American Samoa',\n",
       " 'census district of the United States Virgin Islands',\n",
       " 'district of the United States Virgin Islands']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine instance of classification for Wikidata FIPS 6-4 items from TIGER names not ending in \"County\"\n",
    "wd_fips64['class_list'] = wd_fips64.classes.apply(lambda x: x.split(','))\n",
    "list(wd_fips64[wd_fips64.GEOID.isin(merged_counties_states[~merged_counties_states.NAME.str.endswith('County')].GEOID)][['class_list']].explode('class_list').class_list.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd_matches = wd_fips64[wd_fips64.GEOID.isin(merged_counties_states.GEOID)]\n",
    "wd_matches = wd_matches.drop_duplicates(subset=\"GEOID\", keep=\"first\")\n",
    "\n",
    "merged_counties_wd = pd.merge(\n",
    "    left=merged_counties_states,\n",
    "    right=wd_matches[[\"item\", \"itemDescription\", \"GEOID\"]],\n",
    "    how=\"left\",\n",
    "    on=\"GEOID\"\n",
    ")\n",
    "\n",
    "merged_counties_wd['kb_state_qid'] = merged_counties_wd.st.apply(lambda x: x.split('/')[-1])\n",
    "merged_counties_wd['wd_rel_id'] = merged_counties_wd.item.apply(lambda x: x.split('/')[-1] if isinstance(x, str) else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Items\n",
    "\n",
    "There is a little bit of a functional question here on whether we should run this kind of blending operation in building items. It might be cleaner to simply work on one individual source at a time, using it to present only what it knows individually at the point of integration into a knowledgebase. Each one of these that we run will consult what's already in the knowledgebase to decide what to do next. The next time it runs, it may be able to do a little bit more because some other process has run to bank its own information.\n",
    "\n",
    "In this process, we've done that to some extent - linking to state records already in our knowledgebase. If those records weren't there, and this process ran first, the most notable \"issue\" would have been an inability to create functionally unique labels for these items (because the county FIPS source in the TIGER data files does not include state name). We also consult Wikidata on the way in with these records. That is also something that is going to potentially need to be re-run over time. The only thing we are doing at this stage with Wikidata consultation is bringing in a convenient description and nailing down the link via FIPS code so we can exploit the link to a Wikidata item with additional information about counties more readily later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I goofed up one thing so I needed to see what was already in the knowledgebase\n",
    "query_existing_fips = \"\"\"\n",
    "PREFIX wdt: <https://edji-knows.wikibase.cloud/prop/direct/>\n",
    "\n",
    "SELECT ?fips_code\n",
    "WHERE {\n",
    "    ?item wdt:P31 ?fips_code .\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "existing_fips = sparql_query(\n",
    "    endpoint=os.environ['SPARQL_ENDPOINT_URL'],\n",
    "    query=query_existing_fips,\n",
    "    output=\"dataframe\"\n",
    ")\n",
    "\n",
    "missing_records = merged_counties_wd[~merged_counties_wd.GEOID.isin(existing_fips.fips_code)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiger_refs = References()\n",
    "ref_tiger = datatypes.Item(\n",
    "    prop_nr=properties['data source'],\n",
    "    value=datasources['U.S. Census Bureau TIGER Data Files']\n",
    ")\n",
    "tiger_refs.add(ref_tiger)\n",
    "\n",
    "for index, row in missing_records.iterrows():\n",
    "    print(\"PROCESSING:\", row.label)\n",
    "    prov_statement=\"Added new county record from TIGER source matched to existing state item\"\n",
    "\n",
    "    item = wbi.item.new()\n",
    "    \n",
    "    item.labels.set('en', row.label)\n",
    "    \n",
    "    # Add some additional alt names\n",
    "    aliases = [\n",
    "        row.NAME,\n",
    "        row.BASENAME,\n",
    "        f\"{row.NAME}, {row.fips_alpha}\"\n",
    "    ]\n",
    "    item.aliases.set('en', aliases)\n",
    "\n",
    "    # Instance of classification\n",
    "    item.claims.add(\n",
    "        datatypes.Item(\n",
    "            prop_nr=properties['instance of'],\n",
    "            value=classes['civil political division'],\n",
    "            references=tiger_refs\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Location in state\n",
    "    item.claims.add(\n",
    "        datatypes.Item(\n",
    "            prop_nr=properties['location'],\n",
    "            value=row.kb_state_qid,\n",
    "            references=tiger_refs\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Coordinate location\n",
    "    item.claims.add(\n",
    "        datatypes.GlobeCoordinate(\n",
    "            prop_nr=properties['coordinate location'],\n",
    "            latitude=row.CENTLAT,\n",
    "            longitude=row.CENTLON,\n",
    "            references=tiger_refs\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # FIPS ID\n",
    "    item.claims.add(\n",
    "        datatypes.ExternalID(\n",
    "            prop_nr=properties['FIPS 6-4'],\n",
    "            value=row.GEOID,\n",
    "            references=tiger_refs\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # GNIS ID\n",
    "    item.claims.add(\n",
    "        datatypes.ExternalID(\n",
    "            prop_nr=properties['GNIS ID'],\n",
    "            value=row.COUNTYNS.lstrip('0'),\n",
    "            references=tiger_refs\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Related wikidata item\n",
    "    if isinstance(row.wd_rel_id, str):\n",
    "        # Borrow description from Wikidata\n",
    "        item.descriptions.set('en', row.itemDescription)\n",
    "        \n",
    "        wd_refs = References()\n",
    "        ref_wd = datatypes.Item(\n",
    "            prop_nr=properties['data source'],\n",
    "            value=datasources['Wikidata']\n",
    "        )\n",
    "        wd_refs.add(ref_wd)\n",
    "        \n",
    "        item.claims.add(\n",
    "            datatypes.ExternalID(\n",
    "                prop_nr=properties['related wikidata item'],\n",
    "                value=row.wd_rel_id,\n",
    "                references=wd_refs\n",
    "            )\n",
    "        )\n",
    "        prov_statement=\"Added new county record from TIGER source matched to existing state item with link to related Wikidata item\"\n",
    "    else:\n",
    "        # Set standard description\n",
    "        item.descriptions.set('en', f'a civil political division in {row.stLabel}')\n",
    "\n",
    "    \n",
    "    item.write(summary=prov_statement)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "72dfe86cf9c941cd84c562b05cb374aa",
  "deepnote_persisted_session": {
   "createdAt": "2023-03-05T20:01:49.619Z"
  },
  "kernelspec": {
   "display_name": ".conda-edji-knows",
   "language": "python",
   "name": "conda-env-.conda-edji-knows-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
